{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('adult.data', header=None)\n",
    "test_set = pd.read_csv('adult.test', skiprows=1, header=None) #do not forget to skip the first row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                  1       2           3   4                    5   \\\n",
       "0  39          State-gov   77516   Bachelors  13        Never-married   \n",
       "1  50   Self-emp-not-inc   83311   Bachelors  13   Married-civ-spouse   \n",
       "2  38            Private  215646     HS-grad   9             Divorced   \n",
       "3  53            Private  234721        11th   7   Married-civ-spouse   \n",
       "4  28            Private  338409   Bachelors  13   Married-civ-spouse   \n",
       "\n",
       "                   6               7       8        9     10  11  12  \\\n",
       "0        Adm-clerical   Not-in-family   White     Male  2174   0  40   \n",
       "1     Exec-managerial         Husband   White     Male     0   0  13   \n",
       "2   Handlers-cleaners   Not-in-family   White     Male     0   0  40   \n",
       "3   Handlers-cleaners         Husband   Black     Male     0   0  40   \n",
       "4      Prof-specialty            Wife   Black   Female     0   0  40   \n",
       "\n",
       "               13      14  \n",
       "0   United-States   <=50K  \n",
       "1   United-States   <=50K  \n",
       "2   United-States   <=50K  \n",
       "3   United-States   <=50K  \n",
       "4            Cuba   <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1       2              3   4                    5   \\\n",
       "0  25     Private  226802           11th   7        Never-married   \n",
       "1  38     Private   89814        HS-grad   9   Married-civ-spouse   \n",
       "2  28   Local-gov  336951     Assoc-acdm  12   Married-civ-spouse   \n",
       "3  44     Private  160323   Some-college  10   Married-civ-spouse   \n",
       "4  18           ?  103497   Some-college  10        Never-married   \n",
       "\n",
       "                   6           7       8        9     10  11  12  \\\n",
       "0   Machine-op-inspct   Own-child   Black     Male     0   0  40   \n",
       "1     Farming-fishing     Husband   White     Male     0   0  50   \n",
       "2     Protective-serv     Husband   White     Male     0   0  40   \n",
       "3   Machine-op-inspct     Husband   Black     Male  7688   0  40   \n",
       "4                   ?   Own-child   White   Female     0   0  30   \n",
       "\n",
       "               13       14  \n",
       "0   United-States   <=50K.  \n",
       "1   United-States   <=50K.  \n",
       "2   United-States    >50K.  \n",
       "3   United-States    >50K.  \n",
       "4   United-States   <=50K.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I notice a few problems immediately:**\n",
    "\n",
    "1. We don’t have a column header for our data.\n",
    "\n",
    "\n",
    "2. There seem to be some unknown values in the fifth row of the test set (the question marks) we need to deal with.\n",
    "\n",
    "\n",
    "3. The target values have periods at the end in the test set but do not in the training set (<=50K. vs. <=50K).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the accompanying dataset description, we can see the column names. Let’s put those in for our train and test first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_labels = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', \n",
    "              'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "             'wage_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.columns = col_labels\n",
    "test_set.columns = col_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age               32561 non-null int64\n",
      "workclass         32561 non-null object\n",
      "fnlwgt            32561 non-null int64\n",
      "education         32561 non-null object\n",
      "education_num     32561 non-null int64\n",
      "marital_status    32561 non-null object\n",
      "occupation        32561 non-null object\n",
      "relationship      32561 non-null object\n",
      "race              32561 non-null object\n",
      "sex               32561 non-null object\n",
      "capital_gain      32561 non-null int64\n",
      "capital_loss      32561 non-null int64\n",
      "hours_per_week    32561 non-null int64\n",
      "native_country    32561 non-null object\n",
      "wage_class        32561 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16281 entries, 0 to 16280\n",
      "Data columns (total 15 columns):\n",
      "age               16281 non-null int64\n",
      "workclass         16281 non-null object\n",
      "fnlwgt            16281 non-null int64\n",
      "education         16281 non-null object\n",
      "education_num     16281 non-null int64\n",
      "marital_status    16281 non-null object\n",
      "occupation        16281 non-null object\n",
      "relationship      16281 non-null object\n",
      "race              16281 non-null object\n",
      "sex               16281 non-null object\n",
      "capital_gain      16281 non-null int64\n",
      "capital_loss      16281 non-null int64\n",
      "hours_per_week    16281 non-null int64\n",
      "native_country    16281 non-null object\n",
      "wage_class        16281 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "test_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s check to see if pandas has identified any of these missing values.\n",
    "\n",
    "Haaa!, There don’t seem to be any. According to the accompanying notes(see the adult.names file), the original datasets had 32561 in train and 16281 with test. However, unknowns are included and have been labeled with a question mark (?). The test results documented were done after removing all unknowns. Therefore, to see if we can beat their best results, we need to remove the same unknown rows.\n",
    "\n",
    "If we do so, we should have 30162 in train and 15060 in test. Let’s see if we can remove all of these unknown rows.\n",
    "\n",
    "It turns out the question mark was actually entered with a space first. Let’s do a simple test to see what would happen if we dropped all rows that contain an unknown marked with a `‘ ?’`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30162, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.replace(' ?', np.nan).dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15060, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.replace(\" ?\", np.nan).dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These must be our missing rows since the numbers add up now if we drop them. Let’s apply this change to our test and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nomissing = train_set.replace(' ?', np.nan).dropna()\n",
    "test_nomissing = test_set.replace(' ?', np.nan).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have taken care of the missing value problem, we still have an issue with the target income thresholds being encoded slightly differently in test vs. train. We need these to match up appropriately, so we are going to need to fix either the test or training set to make them match up. Let’s replace all of the `‘<=50K.’` with `‘<=50K’` and the same for `‘>50K.’` with `‘>50K’`, so essentially, we are just dropping the periods. This is also encoded with a space so include this in the string. We can use the replace method from pandas to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nomissing['wage_class'] = test_nomissing.wage_class.replace(\n",
    "    {' <=50K.': ' <=50K', ' >50K.':' >50K'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nomissing['wage_class'] = train_nomissing.wage_class.replace(\n",
    "    {' <=50K.': ' <=50K', ' >50K.':' >50K'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the unique values for `wage_class`, we can see they match now in the both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' >50K'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nomissing.wage_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' >50K'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nomissing.wage_class.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one thing we need to do, however, before applying XGBoost. We need to make sure all categorical variables encoded as a string is turned into a numerical variable, as **xgboost does not accept categorical varibles**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The question is how to convert categorical features (especially nominal features) to numerical features**\n",
    "\n",
    "Let's discuss some of the popular methods including their pros and cons and use fullness of each method in a separate notebook. Namely : **Concepts_of_TS(mean)_Encoding**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Encoding of Categorical Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, combine them together into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_set = pd.concat([train_nomissing, test_nomissing], axis = 0) # Stacks them vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45222 entries, 0 to 16280\n",
      "Data columns (total 15 columns):\n",
      "age               45222 non-null int64\n",
      "workclass         45222 non-null object\n",
      "fnlwgt            45222 non-null int64\n",
      "education         45222 non-null object\n",
      "education_num     45222 non-null int64\n",
      "marital_status    45222 non-null object\n",
      "occupation        45222 non-null object\n",
      "relationship      45222 non-null object\n",
      "race              45222 non-null object\n",
      "sex               45222 non-null object\n",
      "capital_gain      45222 non-null int64\n",
      "capital_loss      45222 non-null int64\n",
      "hours_per_week    45222 non-null int64\n",
      "native_country    45222 non-null object\n",
      "wage_class        45222 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "combined_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_class_binary = pd.get_dummies(combined_set['wage_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_set['wage_class']= wage_class_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = combined_set.groupby('workclass')['wage_class'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workclass\n",
       " Federal-gov         0.609531\n",
       " Local-gov           0.704839\n",
       " Private             0.782298\n",
       " Self-emp-inc        0.445930\n",
       " Self-emp-not-inc    0.721022\n",
       " State-gov           0.732785\n",
       " Without-pay         0.904762\n",
       "Name: wage_class, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_colums = ['workclass','education','marital_status','occupation','relationship',\n",
    "                 'race','sex','native_country',]\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_feature in cat_colums:\n",
    "    combined_set[cat_feature] = combined_set[cat_feature].map(combined_set.groupby(cat_feature)['wage_class'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>wage_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.732785</td>\n",
       "      <td>77516</td>\n",
       "      <td>0.580185</td>\n",
       "      <td>13</td>\n",
       "      <td>0.951980</td>\n",
       "      <td>0.863538</td>\n",
       "      <td>0.895061</td>\n",
       "      <td>0.737629</td>\n",
       "      <td>0.687523</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.746973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.721022</td>\n",
       "      <td>83311</td>\n",
       "      <td>0.580185</td>\n",
       "      <td>13</td>\n",
       "      <td>0.545761</td>\n",
       "      <td>0.520889</td>\n",
       "      <td>0.544252</td>\n",
       "      <td>0.737629</td>\n",
       "      <td>0.687523</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.746973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>0.782298</td>\n",
       "      <td>215646</td>\n",
       "      <td>0.836569</td>\n",
       "      <td>9</td>\n",
       "      <td>0.895982</td>\n",
       "      <td>0.934018</td>\n",
       "      <td>0.895061</td>\n",
       "      <td>0.737629</td>\n",
       "      <td>0.687523</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.746973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>0.782298</td>\n",
       "      <td>234721</td>\n",
       "      <td>0.945028</td>\n",
       "      <td>7</td>\n",
       "      <td>0.545761</td>\n",
       "      <td>0.934018</td>\n",
       "      <td>0.544252</td>\n",
       "      <td>0.873699</td>\n",
       "      <td>0.687523</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.746973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>0.782298</td>\n",
       "      <td>338409</td>\n",
       "      <td>0.580185</td>\n",
       "      <td>13</td>\n",
       "      <td>0.545761</td>\n",
       "      <td>0.549933</td>\n",
       "      <td>0.514108</td>\n",
       "      <td>0.873699</td>\n",
       "      <td>0.886424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.744361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt  education  education_num  marital_status  \\\n",
       "0   39   0.732785   77516   0.580185             13        0.951980   \n",
       "1   50   0.721022   83311   0.580185             13        0.545761   \n",
       "2   38   0.782298  215646   0.836569              9        0.895982   \n",
       "3   53   0.782298  234721   0.945028              7        0.545761   \n",
       "4   28   0.782298  338409   0.580185             13        0.545761   \n",
       "\n",
       "   occupation  relationship      race       sex  capital_gain  capital_loss  \\\n",
       "0    0.863538      0.895061  0.737629  0.687523          2174             0   \n",
       "1    0.520889      0.544252  0.737629  0.687523             0             0   \n",
       "2    0.934018      0.895061  0.737629  0.687523             0             0   \n",
       "3    0.934018      0.544252  0.873699  0.687523             0             0   \n",
       "4    0.549933      0.514108  0.873699  0.886424             0             0   \n",
       "\n",
       "   hours_per_week  native_country  wage_class  \n",
       "0              40        0.746973           1  \n",
       "1              13        0.746973           1  \n",
       "2              40        0.746973           1  \n",
       "3              40        0.746973           1  \n",
       "4              40        0.744361           1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45222 entries, 0 to 16280\n",
      "Data columns (total 15 columns):\n",
      "age               45222 non-null int64\n",
      "workclass         45222 non-null float64\n",
      "fnlwgt            45222 non-null int64\n",
      "education         45222 non-null float64\n",
      "education_num     45222 non-null int64\n",
      "marital_status    45222 non-null float64\n",
      "occupation        45222 non-null float64\n",
      "relationship      45222 non-null float64\n",
      "race              45222 non-null float64\n",
      "sex               45222 non-null float64\n",
      "capital_gain      45222 non-null int64\n",
      "capital_loss      45222 non-null int64\n",
      "hours_per_week    45222 non-null int64\n",
      "native_country    45222 non-null float64\n",
      "wage_class        45222 non-null uint8\n",
      "dtypes: float64(8), int64(6), uint8(1)\n",
      "memory usage: 5.2 MB\n"
     ]
    }
   ],
   "source": [
    "combined_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our features encoded, we need to split these back into their original train/test sizes. Since they haven’t been shuffled we just need to retrieve the same indices as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = combined_set[:train_nomissing.shape[0]] # Up to the last initial training set row\n",
    "\n",
    "final_test = combined_set[train_nomissing.shape[0]:] # Past the last initial training set row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now finally start playing around with XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model Setup and Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have our target value inside our train and test frames that needs to be separated from the feature vectors we will be feeding into XGBoost. Let’s get those now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = final_train.pop('wage_class')\n",
    "y_test = final_test.pop('wage_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the libraries we will need to do grid search for XGBoost. Fortunately for those of us used to sklearn’s API, XGBoost is compatible with this, so we can still utilize the traditional GridSearch with XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all of the available parameters that can be tuned in XGBoost, have a look at the [parameter documentation](https://xgboost.readthedocs.io/en/latest/parameter.html). This should help you better understand the choices I am making to start off our first grid search. I am going to start tuning on the maximum depth of the trees first, along with the min_child_weight, which is very similar to min_samples_split in sklearn’s version of gradient boosted trees. We set the objective to ‘binary:logistic’ since this is a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\n",
    "ind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic'}\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                            cv_params, \n",
    "                             scoring = 'accuracy', cv = 5, n_jobs = -1,return_train_score=True) \n",
    "# Optimize for accuracy since that is the metric used in the Adult Data Set notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s run our grid search with 5-fold cross-validation and see which parameters perform the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0, silent=True,\n",
       "       subsample=0.8),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'max_depth': [3, 5, 7], 'min_child_weight': [1, 3, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.fit(final_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s check our grid scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([19.08266101, 18.00571833, 17.43481426, 29.51810613, 31.96300235,\n",
       "        29.43433008, 43.8946898 , 42.42584314, 38.4221602 ]),\n",
       " 'std_fit_time': array([0.12033969, 0.85500965, 0.48750676, 2.07876085, 1.29696573,\n",
       "        0.08233173, 1.51797837, 1.43439729, 5.49277112]),\n",
       " 'mean_score_time': array([0.28540201, 0.29020205, 0.28760147, 0.59891748, 0.64901962,\n",
       "        0.58771911, 1.28009634, 0.86642365, 0.77261386]),\n",
       " 'std_score_time': array([0.00714409, 0.0101869 , 0.01100136, 0.06540441, 0.0989848 ,\n",
       "        0.11401031, 0.40294839, 0.0825699 , 0.23528658]),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 5, 5, 5, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 3, 5, 1, 3, 5, 1, 3, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 3, 'min_child_weight': 1},\n",
       "  {'max_depth': 3, 'min_child_weight': 3},\n",
       "  {'max_depth': 3, 'min_child_weight': 5},\n",
       "  {'max_depth': 5, 'min_child_weight': 1},\n",
       "  {'max_depth': 5, 'min_child_weight': 3},\n",
       "  {'max_depth': 5, 'min_child_weight': 5},\n",
       "  {'max_depth': 7, 'min_child_weight': 1},\n",
       "  {'max_depth': 7, 'min_child_weight': 3},\n",
       "  {'max_depth': 7, 'min_child_weight': 5}],\n",
       " 'split0_test_score': array([0.86242334, 0.8610973 , 0.86076579, 0.85761644, 0.8562904 ,\n",
       "        0.85546163, 0.85314106, 0.85214653, 0.85164926]),\n",
       " 'split1_test_score': array([0.86822476, 0.86673297, 0.86673297, 0.85993701, 0.85695342,\n",
       "        0.85827946, 0.84999171, 0.85048898, 0.84899718]),\n",
       " 'split2_test_score': array([0.87004807, 0.87054533, 0.86640146, 0.86258909, 0.86176032,\n",
       "        0.86093154, 0.85728493, 0.85877673, 0.85977126]),\n",
       " 'split3_test_score': array([0.87383952, 0.87052387, 0.87234748, 0.86389257, 0.86256631,\n",
       "        0.86240053, 0.85908488, 0.85875332, 0.85858753]),\n",
       " 'split4_test_score': array([0.86751782, 0.87033659, 0.86950754, 0.86403581, 0.86254353,\n",
       "        0.86138286, 0.85557951, 0.8554137 , 0.85740342]),\n",
       " 'mean_test_score': array([0.86841058, 0.86784696, 0.86715072, 0.86161395, 0.86002254,\n",
       "        0.859691  , 0.85501625, 0.85511571, 0.85528148]),\n",
       " 'std_test_score': array([0.00371039, 0.00367281, 0.00384877, 0.00248197, 0.00279982,\n",
       "        0.00251491, 0.00318693, 0.00337491, 0.00420194]),\n",
       " 'rank_test_score': array([1, 2, 3, 4, 5, 6, 9, 8, 7]),\n",
       " 'split0_train_score': array([0.89489826, 0.89274317, 0.89013221, 0.93907746, 0.92834349,\n",
       "        0.91968171, 0.98362966, 0.96721787, 0.95279539]),\n",
       " 'split1_train_score': array([0.8945667 , 0.89199718, 0.89216296, 0.940528  , 0.92908948,\n",
       "        0.92092503, 0.98553608, 0.96812964, 0.95349994]),\n",
       " 'split2_train_score': array([0.89514692, 0.89266028, 0.89062953, 0.94040366, 0.92817771,\n",
       "        0.92088358, 0.98416843, 0.96767375, 0.95383149]),\n",
       " 'split3_train_score': array([0.89573145, 0.89378367, 0.89117281, 0.93816826, 0.9280978 ,\n",
       "        0.92018235, 0.98400332, 0.96676337, 0.95424782]),\n",
       " 'split4_train_score': array([0.89482409, 0.89329079, 0.89121876, 0.93924827, 0.9264846 ,\n",
       "        0.91972981, 0.98383822, 0.96746923, 0.95292363]),\n",
       " 'mean_train_score': array([0.89503348, 0.89289502, 0.89106325, 0.93948513, 0.92803862,\n",
       "        0.9202805 , 0.98423514, 0.96745077, 0.95345965]),\n",
       " 'std_train_score': array([0.00039498, 0.00060508, 0.00067831, 0.00088179, 0.00085265,\n",
       "        0.00053862, 0.00067451, 0.00045556, 0.00054583])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0, silent=True,\n",
       "       subsample=0.8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868410582852596"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3, 'min_child_weight': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first hyperparameter combination performed best and we already beat our target of 85.95% accuracy(see the adult.names file) in our cross-validation! please try optimizing some other hyperparameters now to see if we can beat a mean of 86.84% accuracy.\n",
    "\n",
    "Hint: play around with subsampling along with lowering the learning rate to see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {'learning_rate': [0.1, 0.01], 'subsample': [0.7,0.8,0.9]}\n",
    "\n",
    "ind_params = {'n_estimators': 1000, 'seed':0, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth': 3, 'min_child_weight': 1}\n",
    "\n",
    "\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), cv_params, \n",
    "                             scoring = 'accuracy', cv = 5, n_jobs = -1,return_train_score=True)\n",
    "\n",
    "optimized_GBM.fit(final_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, check the grid scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_GBM.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_GBM.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it doesn’t look like we can improve on this. However, we may be able to optimize a little further by utilizing XGBoost’s built-in cv which allows early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the CV testing performed earlier, we want to utilize the following parameters:\n",
    "\n",
    "- Learning_rate (eta) = 0.1\n",
    "\n",
    "\n",
    "- Subsample, colsample_bytree = 0.8\n",
    "\n",
    "\n",
    "- Max_depth = 3\n",
    "\n",
    "\n",
    "- Min_child_weight = 1\n",
    "\n",
    "There are a few other parameters we could tune in theory to squeeze out further performance, but this is a good enough starting point.\n",
    "\n",
    "To **increase the performance of XGBoost’s speed** through many iterations of the training set, and since from now on we are using only **Python's Native XGBoost API** and not **sklearn’s xgboost API** anymore(skleran does not have early stopping facility), we can create a DMatrix. This sorts the data initially to optimize for XGBoost when it builds trees, making the algorithm more efficient. This is especially helpful when you have a very large number of training examples. To create a DMatrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbdmat = xgb.DMatrix(final_train, y_train) # Create our DMatrix to make XGBoost more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s specify our parameters (with slightly different syntax in some places for the **Pythons native XGBoost  API**) and set our early stopping criteria. For now, let’s be aggressive with the stopping and say we don’t want the accuracy to improve for at least 100 new trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "our_params = {'eta': 0.1, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth':3, 'min_child_weight':1} \n",
    "# Grid Search CV optimized settings\n",
    "\n",
    "cv_xgb = xgb.cv(params = our_params, dtrain = xgbdmat, num_boost_round = 3000, nfold = 5,\n",
    "                metrics = ['error'], # Make sure you enter metrics inside a list or you may encounter issues!\n",
    "                early_stopping_rounds = 100) # Look for early stopping that minimizes error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at our CV results to see how accurate we were with these settings. The output is automatically saved into a pandas dataframe for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xgb.tail(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Will train until cv error hasn't decreased in 100 rounds.\n",
    "Stopping. Best iteration: 443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our CV test error at this number of iterations is 11.6388%, or 88.3612% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our best settings, let’s create this as an xgboost object model that we can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "our_params = {'eta': 0.1, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth':3, 'min_child_weight':1} \n",
    "\n",
    "final_xgb_model = xgb.train(our_params, xgbdmat, num_boost_round = 432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_xgb_model.save_model('final_xgb_model0001.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and its feature map can also be dumped to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump model\n",
    "final_xgb_model.dump_model('final_xgb_model001.raw.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our XGB model object, we can then plot our feature importances using a built-in method. This is similar to the feature importances found in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(final_xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will tell us which features were most important in the series of trees. The `**fnlwgt**` feature seems to have the most importance. Filing `capital_gains` was also important, which makes sense given that only those with greater incomes have the ability to invest. `race` and `sex` were not as important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Performance on Test Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has now been tuned using cross-validation grid search through the sklearn API and early stopping through the built-in XGBoost API. Now, we can see how it finally performs on the test set. Does it match our CV performance? First, create another DMatrix (this time for the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdmat = xgb.DMatrix(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = final_xgb_model.predict(testdmat) # Predict using our testdmat\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the predict function for XGBoost outputs probabilities by default and not actual class labels. To calculate accuracy we need to convert these to a 0/1 label. We will set 0.5 probability as our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred_prob > 0.5] = 1\n",
    "y_pred[y_pred_prob <= 0.5] = 0\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred, y_test), 1-accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Summary\n",
    "In this post, we explored some of the basic functionality involving the XGBoost library. We also learned how it works and why it performs faster than other gradient boosting libraries do. As a test, we used it on an example dataset of US incomes, beating the performance of other documented models for the dataset with very little effort. We were also able to investigate feature importances to see which features were influencing the model most.We also measured the performance of our XGB Binary Classier using various metrics avaliable in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
